# Results and Discussion {#results}

The previous chapters described all simulations and experiments that have been done in order to compare the classification performances of all combinations of classification methods and types of variable scaling that are considered in this study. The four classification methods, as described in Chapter \@ref(methods), are $\ell_1$ linear regression, $\ell_1$ logistic regression, $\ell_1$ support vector machine, and LESS. The five types of variable scaling, as described in Chapter \@ref(scaling), are $x$ (no scaling), $z$ (standardisation), $\mu_k$ (LESS scaling with class-specific means), $\mu_k \sigma^2_k$ (LESS scaling with class-specific means and variances), and $\mu_k \bar{\sigma}^2$ (LESS scaling with class-specific means and pooled variance). This chapter compares the performances of these classifiers and discusses the differences that we observed in all experiments. First, we look at the results of the simulation studies (Chapter \@ref(simulations)), after which the results from the analyses on the real datasets (Chapter \@ref(experiments)) are discussed.

## Results simulation studies

### Simulation 1

The first simulation study tested the influence of an increasing number of variables on the classification performance of the different classifiers. For the first variable in the dataset the data for the two classes was simulated from two normal distributions with different means. For all additional variables the data for both classes was simulated from the standard normal distribution, such that theoretically these noise variables are irrelevant for separating the two classes. The number of variables in the dataset was increased from $2$ to $1000$, in $15$ logarithmic steps. The results of this experiment are shown in Figure \@ref(fig:sim1-var1). In general, as the number of noise variables increases, the number of non-zero coefficients in the model increases as well, while the test AUC decreases. For datasets with only a few variables the classifiers have similar performances. However, when the number of variables in the dataset becomes larger, the differences between the classifiers become more apparent. Overall, $\ell_1$ logistic regression performs best, with on average the lowest model dimensionality and the highest AUC. In terms of model dimensionality, lasso regression performs very similar, but the AUC is a bit lower. LESS includes slightly more dimensions in the model and results in an average AUC close to that of the lasso regression. The $\ell_1$ SVM performs by far the worst. The model dimensionality increases steeply, while additionally the AUC deteriorates quickly. When looking at the different types of variable scaling, the LESS scaling with class-specific means and variances $\left(\mu_k \sigma^2_k\right)$ performs worst for each classification method. For each classification method this scaling method gives the lowest AUC value, while also selecting the most number of variables. The other scaling methods perform similarly to each other. At first sight this deviation seems odd, since the variance of the distributions from which both classes were simulated was set to $1$. Therefore, the $\mu_k \sigma^2_k$ scaling (Equation \@ref(eq:scalinglessstd)) should theoretically result in the same values as the $\mu_k \bar{\sigma}^2$ scaling (Equation \@ref(eq:scalinglessstd2)). However, the class-specific variance of the former method is estimated based on the $50$ samples per class, while the pooled variance of the latter method is estimated based on the $100$ samples from both classes. Generally, the more samples, the more accurate the estimations of the mean and variance. Therefore, it is likely that the class-specific variances of $\mu_k \sigma^2_k$ deviate more from the theoretical variance of the distribution from which the datapoints are sampled, as compared to the pooled variance of $\mu_k \bar{\sigma}^2$. Because of this, the estimated variance of class $1$ differs from the estimated variance of class $2$, leading to a larger difference between the two fractions in Equation \@ref(eq:scalinglessstd), which is not the case in Equation \@ref(eq:scalinglessstd2). Therefore, the values of the scaled datapoints after $\mu_k \sigma^2_k$ scaling are larger, leading to smaller regression coefficients, which in turn allows for more variables being selected in the classification model. And since these variables contain random noise, the addition of them in the classification model results in worse predictions and a lower AUC value for the test dataset. LESS scaling with with class-specific means ($\mu_k$) assumes the same class variances. Therefore the results of the classifiers with this type of scaling perform similar to those with $\mu_k \bar{\sigma}^2$ scaling.

For this simulation $50$ samples were generated for each of the two classes. In the first (meaningful) dimension, samples from class $1$ were generated from a normal distribution with $\mu_1 = -0.5$, while samples from class $2$ were generated from a normal distribution with $\mu_2 = 0.5$. The simulation was also done with $\mu_1 = -1$ and $\mu_2 = 1$. With the samples from the classes further away from each other, there was less overlap between the classes, which reduced the difficulty of the classification problem. Hence, the classification performances of the different classifiers were very similar, with fewer variables included in the models and higher prediction accuracy and AUC on the test set. In order to make the classification problem more difficult and to better distinguish the performance of each classifier, the class means for the first variable in this and all other simulations were set closer to each other (i.e. $\mu_1 = -0.5$ and $\mu_2 = 0.5$). 

The simulation was also done with different values for the variance of the noise variables. Instead of simulating the noise variables from $N(\mu = 0, \sigma^2 = 1)$, the simulation was repeated with noise variables simulated from $N(\mu = 0, \sigma^2 = 0.1)$ and from $N(\mu = 0, \sigma^2 = 10)$. The results of these simulations are shown in Appendix \@ref(appendix-a) and [the online dashboard](https://github.com/vissermachiel/More-with-LESS). The simulation with the noise variance set to $0.1$ gives very similar results as described above, while the simulation with the noise variance set to $10$ results in quick deterioration in model sparseness and AUC for the classifiers in combination with scaling methods that do not include variance scaling ($x$ and $\mu_k$). When the variance of the noise becomes larger, it becomes more likely that there is by chance some difference between the means of the two classes in the training dataset. Therefore, more variables will appear important for the separation problem and will be included in the classification models. This will of course reduce the AUC for predictions on the test dataset.

### Simulation 2

The second simulation study tested the influence of an increasing variance of the noise variables on the performance of the different classifiers, while the number of variables was fixed at either $2$, $10$, $100$, or $1000$. The noise variance was increased from $0.01$ to $100$ in 17 logarithmic steps. Since this study focuses on the analysis of high-dimensional data and the results for lower-dimensional datasets were similar, we focus the discussion of the results on the simulation with $1000$ variables, shown in Figure \@ref(fig:sim2-p1000). 

The classification performance, both in terms of model dimensionality and AUC, stays constant for all classifiers that make use of some form of variance scaling ($z$, $\mu_k \sigma^2_k$, and $\mu_k \bar{\sigma}^2$). Since these methods correct for the (class) variance, changing the variance in the dataset has no effect on the results. Of these scaling types, the use of class-specific variances ($\mu_k \sigma^2_k$) leads to the worst results. Incidental differences between the class variances for certain variables make the classifiers include these noise variables in their model and worsen the AUC. Classifiers that do not include variance scaling ($x$ and $\mu_k$) seem to break when the variance becomes larger than $1$. The number of variables in these models increases sharply, when increasing the variance from $1$ to $100$. The test AUC values of these classifiers drop quickly to $0.5$ at $\sigma^2 = 10$. 

Of the four classification methods $\ell_1$ support vector machine performs worst. The difference between the other three classification methods is less apparent, but $\ell_1$ logistic regression in combination with either regular standardisation ($z$) or LESS scaling with class-specific means and pooled variance ($\mu_k \bar{\sigma}^2$) gives the highest AUC using the fewest number of variables in the model.

### Simulation 3

The third simulation study tested the influence of increasing correlation between the first two variables on the performance of the different classifiers. This was tested by rotating the datapoints in the first two dimensions counterclockwise around the origin for a rotation angle increasing from $0$ to $\pi / 4$ radians ($0$ to $45$ degrees with $1$ degree increase per step). Since the classes are theoretically only separable in the first variable (all other variables contain random noise), without any rotation only this variable should be included in the model. When increasing the rotation angle, the second variable becomes increasingly relevant for separating the classes and is included more often in the model (see Figures \@ref(fig:sim3-p2-var1) and \@ref(fig:sim3-p10-var1)). Although the second variable is more often included in the classification model when rotating the data, the AUC value does not stay constant. This unexpected result could be ascribed to the noise variables that could additionally be included in the models. Because the focus of this simulation lies on the additional importance of the second variable, the plot only shows the average number of model coefficients of the first two dimensions. It does not take into account possible non-zero model coefficients for the noise variables. When the model correctly includes the first and second variables, the prediction AUC could still decrease in case the model also includes one or more noise variables, while these do not help correctly separate the two classes in the test dataset.

The different scaling methods show very similar results, with the exception of $\mu_k \sigma^2_k$, which includes the second variable more often and gives a lower AUC on the test dataset for the same reason as described in the previous simulation studies. A clearer difference is observed between the classification methods. Both $\ell_1$ logistic regression and $\ell_1$ linear regression are less inclined to include the second variable in the model as compared to $\ell_1$ SVM and LESS. Even when the data has not been rotated yet, meaning that only the first variable should be enough to separate the classes, the latter two methods include the second variable on average $25\%$ of the time. The conservative property of the linear and logistic regression methods leads to better predictions and higher AUC values when the correlation between variables is small (small rotation angle). However, when the correlation (rotation angle) becomes larger, this conservative property opposes the addition of the second variable for these methods and leads to worse predictions. Since the second variable becomes increasingly more important for separating the data of the two classes, failing to include this variable in the model results in lower AUC values for these methods. And therefore, the less conservative SVM and LESS perform better in that scenario than the regression methods. 

The difference in performance between the classification methods can be explained by their different loss functions [@LossFunctions]. An overview of the different loss functions for the used classification methods is shown in Figure \@ref(fig:lossfunctions). Linear regression makes use of a squared loss function in the form of $\left( y_i - f(\boldsymbol{x}_i) \right)^2$. This means that the loss increases quadratically with the difference between the observed ($y_i$) and predicted ($f(\boldsymbol{x}_i)$) values for object $i$. Because this function is symmetric, all objects that do not lie exactly on the regression line contribute to the total loss, regardless of whether they are located on the correct or incorrect side of the separation boundary. Logistic regression uses a logistic loss function: $\log(2)^{-1} \log\left(1 + e^{- y_i f(\boldsymbol{x}_i)}\right)$. 
As can be seen in Figure \@ref(fig:lossfunctions), the logistic loss approaches zero only when the object is located far away from the decision boundary and on the correct side (i.e. when the margin $y_i f(\boldsymbol{x}_i)$ is very large). Both the support vector machine and LESS make use of the Hinge loss function: $\text{max} \{ 0, 1 - y_i f(x_i) \}$. With this function only the datapoints that are located on the wrong side of the decision boundary contribute linearly to the loss. Another popular loss function for classification is the $0-1$ loss, which simply indicates whether a prediction is correct ($\text{loss} = 0$) or incorrect ($\text{loss} = 1$). This loss function is not used for our methods. The shape of the loss functions indicate that the addition of wrong variables to the models is more costly for classification methods that use the squared or logistic loss functions, as compared to classifiers that use the Hinge loss function. This could explain the more conservative property of linear regression and logistic regression, while LESS and SVM are less penalised for including extra variables in their models.

```{r lossfunctions, echo=FALSE, out.width="80%", fig.cap="Different loss functions for datapoint $\\boldsymbol{x}_i$ when $y_i = 1 \\in \\{-1, 1\\}$."}
par(mfrow = c(1, 2))
knitr::include_graphics("Figures/6_Discussion/LossFunctions.png")
```

### Simulation 4

The fourth simulation study tested the influence of the sample size of the train set on the performance of the different classifiers. This was tested by increasing the sample size for each of the two classes from $5$ to $100$ in $20$ logarithmic steps. The results for this simulation, when fixing the number of variables to $10$ with noise variance set to $1$, are shown in Figure \@ref(fig:sim4-p10-var1). Results for other parameter setting are put in Appendix \@ref(appendix-a) and [the online dashboard](https://github.com/vissermachiel/More-with-LESS). The conservative property of the $\ell_1$ linear and $\ell_1$ logistic regression models, as seen in the previous simulation study, makes it less likely to include extra variables in these model. While this leads to high AUC values when the sample size is large, the prediction performance of the models trained on very few samples is considerably lower. Although a small sample size in the training dataset makes it difficult to make good predictions on the test dataset, the $\ell_1$ SVM and LESS handle this much better. These classification methods tend to include more variables in their models. While this results in almost identical AUC values for large sample sizes, the AUC of these methods for small sample sizes are considerably higher than those of the linear and logistic regressions.
Especially the standardised versions (with $z$ scaling) of the linear regression and the logistic regression perform poorly for small sample sizes. This is a remarkable result, since standardisation is used as default method to pre-process the data when using $\ell_1$ linear regression and $\ell_1$ logistic regression [@Hastie:ElementsOfStatisticalLearning]. For a sample size smaller than $50$ ($25$ per class) these classifiers become very unstable, with often no variables included in the model at all. This is also the reason that the `glmnet` function in R [@glmnet] gives a warning when the number of observations per class is small. These classification methods perform much better in combination with the three types of LESS scaling ($\mu_k$, $\mu_k \sigma^2_k$, and $\mu_k \bar{\sigma}^2$). 

### Simulation 5

The fifth simulation study tested the robustness of the different classifiers to non-Gaussian noise. This simulation is similar to the first simulation study, but instead of generating noise variables from the standard normal distribution they are now generated from the standard Cauchy distribution. The Cauchy distribution contains more mass in the tails of the distribution. The dataset will therefore contain more extreme (small or large) values, which makes the inclusion of the correct variables in the model more challenging. The results are shown in Figure \@ref(fig:sim5). As expected, the results are similar to those of Simulation 1, but the number of variables included in the models is about five times larger. However, the model dimensionality stays very low for $\ell_1$ linear regression and $\ell_1$ logistic regression, both in combination with regular standardisation ($z$) and LESS scaling with class-specific means and pooled variance ($\mu_k \bar{\sigma}^2$). LESS in combination with $\mu_k \bar{\sigma}^2$ scaling performs equally well. These five classifiers maintain a very low model dimensionality, when increasing the number of variables with Cauchy noise. While the AUC values for all other classifiers quickly drops to $0.5$, when increasing the number of variables, the AUC for these five classifiers remain unchanged around $0.75$. An unexpected result of this simulation study is the difference in performance between classifiers that use LESS scaling with class-specific means and variances ($\mu_k \sigma^2_k$) and those that use LESS scaling with class-specific means and pooled variance ($\mu_k \bar{\sigma}^2$). This phenomenon was also seen in Simulation 1, where the classifiers in combination with the former scaling method perform poorly, while classifiers combined with the latter scaling method perform very well. This could be the result of the larger mass in the tails of the distributions of the noise variables, which leads to a higher variance in these variables. This is especially the case for the scaling method that uses fewer datapoints for estimating the class-specific variances. As a result, this scaling method leads to more noise variables being included in the model and therefore worse predictions for the test dataset. To gain better intuition for the discrepancies between the $\mu_k \sigma^2_k$ and $\mu_k \bar{\sigma}^2$ scaling methods and the influence of the class variances, we suggest a follow-up simulation study where the variance of the two simulated classes is different.

## Results analyses of real data

After testing the performance of the classifiers in controlled situations with simulated data, they were also tested on eight real-world high-dimensional datasets. The resulting model dimensionality, test AUC, and test accuracy of these analyses are shown in the tables of Figures \@ref(fig:datasets-numbetas), \@ref(fig:datasets-auc), and \@ref(fig:datasets-accuracy), respectively. On average the classifiers performed best on the `SRBCT` dataset, while the `metastasis` dataset (containing the most variables) yielded the worst classification performance. 

In order to easily compare the performance of the different classification methods, the results for each method are calculated by taking the average values over the combinations with all scaling methods for all datasets. The resulting average model dimensionality, test AUC and test accuracy for each method is shown in Table \@ref(tab:methods-average). The average performance of LESS is based on only the combinations with the three LESS scaling methods ($\mu_k$, $\mu_k \sigma^2_k$, $\mu_k \bar{\sigma}^2$). LESS without any data scaling ($x$) or in combination with regular standardisation ($z$) is meaningless, because the LESS classification method is by definition based on the difference between class means (with or without variance correction). When these class differences are not taken into account, the resulting classifier does not contain this key property and is therefore not included in the calculation of the average performance of LESS. Although the combinations of LESS with $x$ and $z$ were included in the analyses, the results were omitted from all figures and tables.

```{r methods-average, echo=FALSE}
knitr::kable(booktabs = TRUE,
             col.names = c("Classification method", "Model dimensionality", "Test AUC", "Test Accuracy"),
             data.frame(Classification_method = c("Linear regression", "Logistic regression", "SVM", "LESS"),
                        Model_dimensionality = c(28.6, 14.0, 23.8, 15.4),
                        Test_AUC = c(0.900, 0.902, 0.922, 0.917),
                        Test_Accuracy = c(83.2, 83.3, 87.2, 86.9)),
             align = "lccc",
             caption = "Average classification performance for all four classification methods: $\\ell_1$ linear regression, $\\ell_1$ logistic regression, $\\ell_1$ support vector machine, and LESS.")
```

As shown in Table \@ref(tab:methods-average) the four classification methods have on average comparable classification performance in terms of AUC on the test set. When looking at the model dimensionality, however, the difference is clearer. Linear regression with $\ell_1$ regularisation performs on average worst of all methods. It includes the most variables in its model, while still resulting in the lowest accuracy and AUC on the test set. The highest prediction accuracy and AUC are obtained by $\ell_1$ SVM, although this is most likely the result of the large number of variables that are included in its model. Both $\ell_1$ logistic regression and LESS use considerably fewer variables, while still making good predictions for unseen data. The slightly larger number of non-zero model coefficients in the LESS classifier is offset by the higher AUC and accuracy obtained by this method as compared to the logistic regression. On average for these datasets LESS is therefore considered the best classification method.

The performance of the different variable scaling methods is also compared by calculating the averages over the combinations with all classification methods for all datasets. The resulting average model dimensionality, test AUC and test accuracy for each method is shown in Table \@ref(tab:scaling-average). The average performances of $x$ and $z$ are based on all classification methods except LESS, for the same reason as described above. The performance of the $\mu_k \sigma^2_k$ scaling method stands out from the rest, yielding a disproportionately greater model dimensionality in addition to much lower AUC and accuracy. By replacing the class-specific variances in the equation by a pooled variance (after subtracting the class-specific means from the data values) we get the $\mu_k \bar{\sigma}^2$ scaling method, which results in the best prediction performance of all. This scaling type yields the highest AUC and accuracy, while using very few variables. Only the $\mu_k$ scaling method has on average fewer variables in its model, but results in lower AUC and accuracy.

```{r scaling-average, echo=FALSE}
knitr::kable(booktabs = TRUE,
             col.names = c("Scaling method", "Model dimensionality", "Test AUC", "Test Accuracy"),
             data.frame(Scaling_method = c("$x$", "$z$", "$\\mu_k$", "$\\mu_k \\sigma^2_k$", "$\\mu_k \\bar{\\sigma}^2$"),
                        Model_dimensionality = c(18.1, 19.2, 14.6, 35.8, 16.3),
                        Test_AUC = c(0.890, 0.930, 0.913, 0.883, 0.933),
                        Test_Accuracy = c(83.2, 86.1, 85.5, 81.7, 88.1)),
             align = "lccc",
             caption = "Average classification performance for all five variable scaling methods: $x$ (no scaling), $z$ (standardisation), $\\mu_k$ (LESS scaling with class-specific means), $\\mu_k \\sigma^2_k$ (LESS scaling with class-specific means and variances), and $\\mu_k \\bar{\\sigma}^2$ (LESS scaling with class-specific means and pooled variance).")
```

Looking at all combinations of classification methods and scaling types, the best classification performance for each classification method is consistently obtained when using the $\mu_k \bar{\sigma}^2$ scaling. Even in combination with $\ell_1$ logistic regression, the LESS scaling with class-specific means and pooled variance outperforms standardisation, which is considered to be the default scaling method for regularised regression models.

## Results increasing sample size of real data

In order to test how the classifiers perform when only limited data is available, the analyses on the eight datasets were repeated for reduced sample sizes of the training datasets. The smallest sample size in this experiment was $8$. The size of this subsample was increased in $10$ logarithmic steps until the total sample size of the training dataset was reached. For each sample size all eighteen classifiers were trained on the same subset of the training dataset and the corresponding classification performances were obtained by testing the models on the same test dataset. The results for the `colon` and `metastasis` datasets are visualised in Figures \@ref(fig:colon-samplesize) and \@ref(fig:metas-samplesize). Figures for the other datasets show similar results and are included in Appendix \@ref(appendix-b) and [the online dashboard](https://github.com/vissermachiel/More-with-LESS). The results of this experiment on real-world datasets are in accordance with the results from the fourth simulation study of Chapter \@ref(simulations). The most striking result is the difference in prediction performance between classifiers that use standardisation ($z$ scaling) and those that use any type of LESS scaling ($\mu_k$, $\mu_k \sigma^2_k$, or $\mu_k \bar{\sigma}^2$). For small sample sizes classifiers using the LESS scaling types reach AUC values that are $0.2$ higher than those that use standardisation. For all classification methods LESS scaling with class-specific means and pooled variance ($\mu_k \bar{\sigma}^2$) gives the best results, both for small and large sample sizes, while including relatively few variables in the model.

## Ideas for future work

During the course of this thesis we came across several ideas for further research. Unfortunately there was no time to dive into these topics during this project. Otherwise it would have taken even longer to finish it. Nevertheless, this provides opportunities for other students or researchers to focus on in future projects. An overview of interesting follow-up topics:

* Instead of using $\ell_1$ regularisation, it would be interesting to apply elastic net regularisation. Since this is a combination of both lasso ($\ell_1$) and ridge ($\ell_2$) regularisation, this could improve the classification performance in scenarios when lasso is too strict. A disadvantage of the elastic net would be the addition of another parameter ($\alpha$) that should be optimised. The value of $\alpha$ determines the proportional contributions of the lasso and ridge penalties.
    
* In this thesis we compared classification methods that used three different loss functions: quadratic loss, logistic loss, and Hinge loss. Other loss functions could also be applied, which combine properties of multiple functions. The Huber loss function for regression models [@HuberLoss], for example, is similar to the squared loss for small margins (small distance between datapoint and decision boundary). But when the margin increases, the function increases linearly instead of quadratically. The Huber loss is therefore less strict in avoiding outliers than the quadratic loss. The Huber loss can also be combined with the Hinge loss, resulting in the modified Huber loss function for classification problems [@ModifiedHuberLoss]. While the Hinge loss increases linearly for all wrongly classified objects, the modified Huber loss function increases quadratically for small margins and linearly for large margins. It is therefore less strict for wrongly classified datapoints that are located close to the decision boundary, but more strict for misclassifications further away from the boundary.
    
* Comparisons with other linear classification methods, such as the na√Øive Bayes classifier, linear discriminant analysis, or decision trees.
    
* In order to get a better understanding of the difference between the different types of variable scaling (especially between $\mu_k \sigma^2_k$ and $\mu_k \bar{\sigma}^2$) we propose a follow-up simulation study, where the variance of one of the classes increases while the variance of the other class stays the same. This way the difference between class variances becomes larger, which has consequences for the scaling of the data when using class-specific variances.
    
* LESS has a constraint that all weights should be larger or equal to zero. An idea for another follow-up study is to allow these weights to be negative too.
