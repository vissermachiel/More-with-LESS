# Conclusion {#conclusion}

The LESS classifier is a good method for two-class linear classification problems for high-dimensional data. As the experiments on both simulated and real world datasets show, this classification method performs competitively with other well-known classification methods, such as the $\ell_1$ regularised versions of linear regression, logistic regression, and support vector machine.

For an increasing number of variables, LESS performs competitively with linear and logistic regression, while SVM performs worst in terms of model dimensionality and AUC value. As a result of the different loss functions, linear regression (squared loss) and logistic regression (logistic loss) tend to include fewer variables in their models than LESS and SVM (both Hinge loss). This conservative property of the regression models becomes clear when an additional variable becomes increasingly more important for separating the classes. The sparser regression models lead to better predictions and higher AUC values when this additional variable is not so important for separating the classes. However, when this variable becomes more relevant, the conservative property prevents the addition of this variable in the model and leads to worse predictions. Since the additional variable becomes increasingly more important for separating the data of the two classes, failing to include this variable in the model results in lower AUC values for these methods. The less conservative LESS performs better in that scenario than both regression methods. Another scenario when LESS outperforms the other classification methods is when the sample size is very small. Since it includes more variables in its model it can better estimate the class distributions and make better predictions. This is an important advantage when the data is very high-dimensional.

LESS makes use of a mapping of the data based on the estimated class means for each variable ($\mu_k$ scaling). Two additional variable scaling methods are proposed that additionally correct for the estimated variances: $\mu_k\sigma^2_k$ (LESS scaling with class-specific means and variances) and $\mu_k\bar{\sigma}^2$ (LESS scaling with class-specific means and pooled variances). For each classification method the $\mu_k\sigma^2_k$ scaling results in the lowest AUC value, while also selecting the most number of variables. The best scaling method is $\mu_k\bar{\sigma}^2$. Scaling the data based on class-specific means and pooled variance results in better classification performance than regular standardisation.

Overall, after testing all combinations of the classification methods and types of variable scaling on both simulated and empirical datasets, the best classification performance is achieved for LESS in combination with $\mu_k\bar{\sigma}^2$ scaling and for $\ell_1$ logistic regression in combination with $\mu_k\bar{\sigma}^2$ scaling.
